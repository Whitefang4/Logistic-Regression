# -*- coding: utf-8 -*-
"""Logistic regg.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QHdKKaGTH17E7UqP5nl7rl6PfBh71Zi0
"""



# Importing Libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import scipy.stats as stat
import statsmodels.api as smf
from sklearn.linear_model import LogisticRegression
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.model_selection import KFold , GridSearchCV, train_test_split, StratifiedKFold
from sklearn.metrics import confusion_matrix as cm, accuracy_score as ac, classification_report as report,\
roc_curve, roc_auc_score , recall_score , precision_score, f1_score

! pip install streamlit -q

# Commented out IPython magic to ensure Python compatibility.
## For the sake of debugging
import pdb

## Data structures
import numpy as np
import pandas as pd

## Visualisation
import seaborn as sns
import matplotlib.pyplot as plt

## Preprocessing
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelBinarizer

## Models
# Tuning parameters
from sklearn.model_selection import GridSearchCV, cross_val_score



## Evaluation
from sklearn.metrics import make_scorer, accuracy_score, confusion_matrix

# Magic method used for displaying images directly in Jupyter Notebook
# %matplotlib inline

# Importing Dataset
raw_data = pd.read_csv('/content/Titanic_train.csv')
raw_data

"""# Data Exploration"""

# print shape of dataset with rows and columns
print(raw_data.shape)
# print top 5 records
raw_data.head()

raw_data.describe()

raw_data.info()

raw_data.isnull().sum()

# List of Numerical Variables
numerical_features=[feature for feature in raw_data.columns if raw_data[feature].dtypes != 'O']

print('Number of numerical variables:', len(numerical_features))

# Visualize the numerical variables
raw_data[numerical_features].head()

year_feature = [] # You need to define year_feature
discrete_feature=[feature for feature in numerical_features if len(raw_data[feature].unique())<25 and feature not in year_feature]
print('Discrete Variables Count: {}'.format(len(discrete_feature)))

continuous_feature = [feature for feature in numerical_features if feature not in discrete_feature and feature not in year_feature]
print('Continuous Feature Count {}'.format(len(continuous_feature)))

# Check for data types before correlation
print(raw_data.dtypes)

# Handle non-numeric columns before correlation
for col in raw_data.columns:
    if raw_data[col].dtype == 'object':
        # Convert to categorical codes if needed
        raw_data[col] = raw_data[col].astype('category').cat.codes
        print(f"Converted column '{col}' to category codes")


# Calculate correlation matrix
correlation_matrix = raw_data.corr()

correlation_matrix

raw_data.corr()

"""# Exploratory Data Analysis"""

fig= plt.figure(figsize=(18, 6))
sns.heatmap(raw_data.corr(), annot=True);
plt.xticks(rotation=45)

for feature in continuous_feature:
    data=raw_data.copy()
    data[feature].hist(bins=25)
    plt.ylabel('Count')
    plt.title(feature)
    plt.show()

for feature in continuous_feature:
    data=raw_data.copy()
    if 0 in data[feature].unique():
        pass
    else:
        data[feature]=np.log(data[feature])
        data[feature].hist(bins=25)
        plt.ylabel('Count')
        plt.title(feature)
        plt.show()

"""Dependencies between data
A key to creating a good model is learning how does the data correlate - what are the dependencies between various attributes.
"""

train_df = raw_data.copy()

"""**Sex**

There is a disparity between survivors depending on the sex - women were more likely to survive.
"""

sns.countplot(x='Survived', hue='Sex', data=train_df)
plt.title('Survivors depending on the sex')

"""**Embarked**

Most of the casualties were embarked in 'S'.
"""

sns.countplot(x='Survived', hue='Embarked', data=train_df)
plt.title('Survivors depending on the emberkment place')

"""**Pclass**

Passengers from the 3rd class were more likely to die during the cruise - they were the most of the casualties. The number of people that survived, depending on the class, was rather even.
"""

sns.countplot(x='Survived', hue='Pclass', data=train_df)
plt.title('Survivors depending on the class')

"""**Age**

Male:

most of the survivors were between ages 20 to 40
Female:

more survivors overall
the difference between survival rate based on age is smaller than in case of men
"""



plt.figure(figsize=(10,6))
sns.violinplot(x='Sex', y='Age', hue='Survived', data=train_df, split=True, inner="quartile")

"""**Fare**

It seems that passengers with cheaper tickets were more likely to die. Binning of this data might be difficult since the data is far from being normally distributed.
"""

survived = train_df[train_df['Survived'] == 1]['Fare']
dead = train_df[train_df['Survived'] == 0]['Fare']

figure = plt.figure(figsize=(24,7))
plt.hist([survived, dead], stacked=True, color=['b','r'], alpha=0.4, bins=40, label=['Survived', 'Dead'])
plt.xlabel('Fare')
plt.ylabel('Number of pa')
plt.legend()

"""# Data Preprocessing"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Load the datasets
train_data = pd.read_csv('/content/Titanic_train.csv')
test_data = pd.read_csv('/content/Titanic_test.csv')

# Combine datasets for consistent preprocessing
combined_data = pd.concat([train_data, test_data], ignore_index=True)

# Separate features (X) and target (y) if present, for the combined data, it will be handled later
if 'Survived' in combined_data.columns:
    X = combined_data.drop('Survived', axis=1)
    y = combined_data['Survived']
else:
    X = combined_data
    y = None

# Drop unnecessary columns
X = X.drop(['PassengerId', 'Ticket', 'Name'], axis=1)

# Identify numerical and categorical features
numerical_features = X.select_dtypes(include=np.number).columns.tolist()
categorical_features = X.select_dtypes(include=['object']).columns.tolist()

# --- Numerical Feature Pipeline ---
numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),  # Handle missing values with median
    ('scaler', StandardScaler())  # Scale numerical features
])

# --- Categorical Feature Pipeline ---
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),  # Handle missing values with most frequent
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # One-hot encode categorical features
])

# --- Combine Pipelines with ColumnTransformer ---
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ],
    remainder='passthrough'  # Specify how to handle remaining columns
)

# Apply the preprocessing to the combined data
X_preprocessed = preprocessor.fit_transform(X)

# Get the names of the preprocessed columns
numerical_columns_preprocessed = numerical_features
categorical_columns_preprocessed = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features)
# Convert numpy array to list
categorical_columns_preprocessed = categorical_columns_preprocessed.tolist()
all_columns_preprocessed = numerical_columns_preprocessed + categorical_columns_preprocessed
#Get the columns from the dataframe which were not transformed
columns_not_transformed = [col for col in X.columns if col not in numerical_features + categorical_features]
#Add the columns not transformed to the names of all the columns preprocessed
all_columns_preprocessed += columns_not_transformed

X_preprocessed_df = pd.DataFrame(X_preprocessed, columns=all_columns_preprocessed)

# Split the data back into train and test sets based on the original indices
train_data_preprocessed = X_preprocessed_df[:len(train_data)]
test_data_preprocessed = X_preprocessed_df[len(train_data):]
if y is not None:
    y_preprocessed = y[:len(train_data)]
    X_train = train_data_preprocessed
    y_train = y_preprocessed
else:
    X_train = train_data_preprocessed
    y_train = None

# Display the first five preprocessed training data
display(X_train.head())

# Split the training set into training and validation sets
X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# Initialize and train the Logistic Regression model
logreg = LogisticRegression(max_iter=1000, random_state=42)
logreg.fit(X_train_split, y_train_split)

# Predict on the validation set
y_pred = logreg.predict(X_val)

# Evaluate the model
accuracy = accuracy_score(y_val, y_pred)
print(f"Validation Accuracy: {accuracy:.4f}")

# Print classification report
print("\nClassification Report:")
print(report(y_val, y_pred))

# Print confusion matrix
cm = confusion_matrix(y_val, y_pred)
print("\nConfusion Matrix:")
print(cm)

# Visualize the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, roc_auc_score

# Assuming you have y_val (true labels) and y_pred_prob (predicted probabilities)
# If you don't have probabilities, you can get them using:
y_pred_prob = logreg.predict_proba(X_val)[:, 1]  # Probability of the positive class


fpr, tpr, thresholds = roc_curve(y_val, y_pred_prob)
roc_auc = roc_auc_score(y_val, y_pred_prob)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], 'k--')  # Random classifier line
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc="lower right")
plt.show()



"""Deployment"""

!wget -q -O - ipv4.icanhazip.com